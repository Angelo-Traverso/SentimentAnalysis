{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atrav\\AppData\\Local\\Temp\\ipykernel_2012\\1908590654.py:17: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPooling2D, Embedding, LSTM, Dense\n",
    "from pyspark.sql import SparkSession\n",
    "import imghdr\n",
    "from pyspark.sql.functions import when, col\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"YelpReviews\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jsc.sc().getExecutorMemoryStatus().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"yelp_academic_dataset_review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|XQfwVwDr-v0ZS3_Cb...|   0|2018-07-07 22:09:11|    0|KU_O5udG6zpxOg-Vc...|  3.0|If you decide to ...|     0|mh_-eMZ6K5RLWhZyI...|\n",
      "|7ATYjTIgM3jUlt4UM...|   1|2012-01-03 15:28:18|    0|BiTunyQ73aT9WBnpR...|  5.0|I've taken a lot ...|     1|OyoGAe7OKpv6SyGZT...|\n",
      "|YjUWPpI6HXG530lwP...|   0|2014-02-05 20:30:30|    0|saUsX_uimxRlCVr67...|  3.0|Family diner. Had...|     0|8g_iMtfSiwikVnbP2...|\n",
      "|kxX2SOes4o-D3ZQBk...|   1|2015-01-04 00:01:03|    0|AqPFMleE6RsU23_au...|  5.0|Wow!  Yummy, diff...|     1|_7bHUi9Uuf5__HHc_...|\n",
      "|e4Vwtrqf-wpJfwesg...|   1|2017-01-14 20:54:15|    0|Sx8TMOWLNuJBWer-0...|  4.0|Cute interior and...|     1|bcjbaE6dDog4jkNY9...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "# Display some data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"text\", \"stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|stars|\n",
      "+--------------------+-----+\n",
      "|If you decide to ...|  3.0|\n",
      "|I've taken a lot ...|  5.0|\n",
      "|Family diner. Had...|  3.0|\n",
      "|Wow!  Yummy, diff...|  5.0|\n",
      "|Cute interior and...|  4.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"label\", when(col(\"stars\") <= 2, 0)    # Negative\n",
    "                             .when(col(\"stars\") == 3, 1)  # Neutral\n",
    "                             .otherwise(2))              # Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                text|stars|label|\n",
      "+--------------------+-----+-----+\n",
      "|If you decide to ...|  3.0|    1|\n",
      "|I've taken a lot ...|  5.0|    2|\n",
      "|Family diner. Had...|  3.0|    1|\n",
      "|Wow!  Yummy, diff...|  5.0|    2|\n",
      "|Cute interior and...|  4.0|    2|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down-Sampling attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check the distribution of classes\n",
    "class_distribution = df.groupBy(\"label\").count().collect()\n",
    "total_count = sum([row['count'] for row in class_distribution])\n",
    "\n",
    "# Calculate the fraction of each class in the original dataset\n",
    "fractions = {row['label']: row['count'] / total_count for row in class_distribution}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate how many samples to take from each class\n",
    "target_samples = 500000\n",
    "samples_per_class = {label: int(fractions[label] * target_samples) for label in fractions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Downsample the dataset\n",
    "# Filter each class separately and sample the required number of entries\n",
    "downsampled_df = df.sampleBy(\"label\", fractions={label: samples_per_class[label] / df.filter(col(\"label\") == label).count() for label in samples_per_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                text|stars|label|\n",
      "+--------------------+-----+-----+\n",
      "|I am a long term ...|  1.0|    0|\n",
      "|This easter inste...|  3.0|    1|\n",
      "|I go to blow bar ...|  5.0|    2|\n",
      "|I recently had di...|  5.0|    2|\n",
      "|I've been to this...|  3.0|    1|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Show the downsampled data\n",
    "downsampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count in downsampled DataFrame: 500234\n"
     ]
    }
   ],
   "source": [
    "# Verify the size of the downsampled DataFrame\n",
    "print(\"Total count in downsampled DataFrame:\", downsampled_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df.createOrReplaceTempView(\"down_df\")\n",
    "positive_counts = spark.sql(\"SELECT COUNT(*) FROM down_df WHERE label=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  335288|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_counts = spark.sql(\"SELECT COUNT(*) FROM down_df WHERE label=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   49301|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neutral_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_counts = spark.sql(\"SELECT COUNT(*) FROM down_df WHERE label=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  115645|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_counts = downsampled_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in downsampled_df.columns])\n",
    "\n",
    "# # Show the count of nulls in each column\n",
    "# null_counts.show()\n",
    "null_count_label = downsampled_df.where(col(\"label\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|text|stars|label|\n",
      "+----+-----+-----+\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count_label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_label = downsampled_df.where(col(\"text\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|text|stars|label|\n",
      "+----+-----+-----+\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count_label.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Down Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_rdd = downsampled_df.select(\"text\").rdd.flatmap(lambda row: row.text)\n",
    "# labels_rdd = downsampled_df.select(\"label\").rdd.flatmap(lambda row: row.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =downsampled_df.select(\"text\")\n",
    "labels = downsampled_df.select(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|I am a long term ...|\n",
      "|This easter inste...|\n",
      "|I go to blow bar ...|\n",
      "|I recently had di...|\n",
      "|I've been to this...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_rdd = downsampled_df.select(\"text\").rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'stars', 'label']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pand_df = downsampled_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500234, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pand_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pand_df[\"text\"]\n",
    "y = pand_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atrav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "C:\\Users\\atrav\\AppData\\Local\\Temp\\ipykernel_2012\\3479617163.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['reviews_without_stopwords'] = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            I am a long term frequent customer of this est...\n",
      "1                            This easter instead of going to Lopez Lake we ...\n",
      "2                            I go to blow bar to get my brows done by natal...\n",
      "3                            I recently had dinner here with my wife over t...\n",
      "4                            I've been to this location many times when I l...\n",
      "                                                   ...                        \n",
      "500230                       I decided to try this place out after Christma...\n",
      "500231                       Food was great (lots of gluten-free options!!!...\n",
      "500232                       Took a colleague here for dinner as we were tr...\n",
      "500233                       It is very rare for a restaurant to be this go...\n",
      "reviews_without_stopwords    0         I long term frequent customer establ...\n",
      "Name: text, Length: 500235, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "pos_tweets = [('I love this car', 'positive'),\n",
    "    ('This view is amazing', 'positive'),\n",
    "    ('I feel great this morning', 'positive'),\n",
    "    ('I am so excited about the concert', 'positive'),\n",
    "    ('He is my best friend', 'positive')]\n",
    "\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "X['reviews_without_stopwords'] = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pand_df = pand_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING URL's\n",
    "import re\n",
    "\n",
    "# Define a regex pattern to match URLs\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "# Define a function to remove URLs from text\n",
    "def remove_urls(text):\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column 'clean_text'\n",
    "pand_df['text'] = pand_df['text'].apply(remove_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non-whitespace characters\n",
    "pand_df = pand_df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Digits\n",
    "pand_df = pand_df.replace(to_replace=r'\\d', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atrav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "pand_df['text'] = pand_df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a function to perform stemming on the 'text' column\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Define a function to perform stemming on the 'text' column\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column 'stemmed_text'\n",
    "pand_df['stemmed_messages'] = pand_df['text'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pand_df[\"stemmed_messages\"].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pand_df['stemmed_messages'].values\n",
    "labels = pand_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts: [list(['i', 'am', 'a', 'long', 'term', 'frequent', 'custom', 'of', 'thi', 'establish', 'i', 'just', 'went', 'in', 'to', 'order', 'take', 'out', 'app', 'and', 'wa', 'told', 'theyr', 'too', 'busi', 'to', 'do', 'it', 'realli', 'the', 'place', 'is', 'mayb', 'half', 'full', 'at', 'best', 'doe', 'your', 'dick', 'reach', 'your', 'ass', 'ye', 'go', 'fuck', 'yourself', 'im', 'a', 'frequent', 'custom', 'and', 'great', 'tipper', 'glad', 'that', 'kanella', 'just', 'open', 'never', 'go', 'back', 'to', 'dmitri'])\n",
      " list(['thi', 'easter', 'instead', 'of', 'go', 'to', 'lopez', 'lake', 'we', 'went', 'to', 'lo', 'padr', 'nation', 'forest', 'which', 'is', 'realli', 'pretti', 'but', 'if', 'you', 'go', 'to', 'white', 'rock', 'the', 'staff', 'need', 'to', 'cut', 'down', 'all', 'the', 'dead', 'grass', 'that', 'invad', 'the', 'rock', 'and', 'the', 'water', 'i', 'would', 'wish', 'the', 'staff', 'would', 'also', 'clean', 'or', 'get', 'rid', 'of', 'the', 'dead', 'grass', 'that', 'also', 'live', 'by', 'the', 'water', 'the', 'water', 'is', 'realli', 'green', 'and', 'dirti', 'lo', 'padr', 'nation', 'forest', 'staff', 'need', 'to', 'work', 'hard', 'to', 'maintain', 'thi', 'forest', 'look', 'pretti', 'and', 'not', 'like', 'a', 'dumpster', 'even', 'cachuma', 'lake', 'look', 'like', 'they', 'put', 'a', 'bit', 'more', 'effort'])\n",
      " list(['i', 'go', 'to', 'blow', 'bar', 'to', 'get', 'my', 'brow', 'done', 'by', 'natali', 'brow', 'specialist', 'which', 'i', 'highli', 'recommend', 'she', 'is', 'great', 'doe', 'a', 'great', 'job', 'on', 'my', 'eyebrow', 'but', 'then', 'i', 'got', 'a', 'blow', 'by', 'victoria', 'wow', 'i', 'wa', 'impress', 'i', 'have', 'thin', 'straight', 'dead', 'hair', 'and', 'she', 'left', 'me', 'with', 'the', 'biggest', 'volum', 'ive', 'ever', 'had', 'tri', 'anoth', 'girl', 'but', 'didnt', 'like', 'it', 'as', 'much', 'so', 'victoria', 'will', 'be', 'my', 'girl', 'for', 'ever', 'veri', 'beauti', 'clean', 'place'])\n",
      " list(['i', 'recent', 'had', 'dinner', 'here', 'with', 'my', 'wife', 'over', 'the', 'weekend', 'and', 'could', 'not', 'have', 'been', 'more', 'pleas', 'our', 'meal', 'wa', 'excel', 'my', 'wife', 'and', 'i', 'were', 'astound', 'by', 'how', 'quickli', 'our', 'food', 'came', 'out', 'everyth', 'tast', 'fresh', 'and', 'homemad', 'which', 'we', 'both', 'appreci', 'the', 'onli', 'problem', 'wa', 'tri', 'to', 'pick', 'someth', 'from', 'the', 'menu', 'as', 'there', 'were', 'too', 'mani', 'delici', 'sound', 'item', 'to', 'choos', 'from', 'we', 'will', 'definit', 'be', 'return', 'to', 'tri', 'more', 'item', 'cant', 'wait', 'to', 'see', 'what', 'the', 'next', 'chef', 'special', 'will', 'be'])\n",
      " list(['ive', 'been', 'to', 'thi', 'locat', 'mani', 'time', 'when', 'i', 'live', 'in', 'the', 'area', 'and', 'although', 'it', 'is', 'a', 'chain', 'and', 'not', 'exactli', 'sexi', 'it', 'doe', 'the', 'job', 'on', 'mani', 'occas', 'if', 'you', 'need', 'an', 'oil', 'chang', 'thi', 'place', 'is', 'ok', 'if', 'you', 'need', 'a', 'state', 'inspect', 'thi', 'place', 'doe', 'it', 'the', 'key', 'is', 'get', 'there', 'when', 'they', 'are', 'not', 'busi', 'or', 'els', 'you', 'will', 'wait', 'again', 'noth', 'fanci', 'here', 'a', 'pep', 'boy', 'is', 'a', 'pep', 'boy', 'but', 'all', 'in', 'all', 'decent', 'stuff'])]\n",
      "Sample labels: [0 1 2 2 1]\n",
      "Type of texts: <class 'numpy.ndarray'>\n",
      "Type of labels: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample texts:\", texts[:5])\n",
    "print(\"Sample labels:\", labels[:5])\n",
    "print(\"Type of texts:\", type(texts))\n",
    "print(\"Type of labels:\", type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of texts array: (500234,)\n",
      "Shape of labels array: (500234,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of texts array:\", texts.shape)\n",
    "print(\"Shape of labels array:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty texts: 0\n",
      "Number of empty labels: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of empty texts:\", sum(pd.isnull(pand_df['stemmed_messages'])))\n",
    "print(\"Number of empty labels:\", sum(pd.isnull(pand_df['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)  # Limit to top 5000 words\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequences: [[3, 152, 4, 228, 1598, 964, 146, 8, 17, 731, 3, 48, 115, 11, 5, 40, 122, 41, 854, 2, 6, 180, 502, 100, 170, 5, 76, 7, 66, 1, 28, 9, 354, 353, 304, 30, 89, 444, 58, 4981, 1453, 58, 1910, 545, 42, 2597, 744, 96, 4, 964, 146, 2, 32, 585, 13, 48, 233, 110, 42, 52, 5], [17, 4109, 454, 8, 42, 5, 2825, 14, 115, 5, 1720, 2322, 4227, 63, 9, 66, 160, 18, 44, 20, 42, 5, 587, 1040, 1, 95, 120, 5, 434, 184, 43, 1, 1757, 2735, 13, 1, 1040, 2, 1, 331, 3, 54, 431, 1, 95, 54, 70, 198, 57, 39, 3111, 8, 1, 1757, 2735, 13, 70, 276, 81, 1, 331, 1, 331, 9, 66, 470, 2, 809, 1720, 2322, 4227, 95, 120, 5, 118, 380, 5, 1814, 17, 4227, 86, 160, 2, 23, 45, 4, 78, 2825, 86, 45, 16, 278, 4, 222, 80, 1556], [3, 42, 5, 1551, 141, 5, 39, 12, 2912, 277, 81, 2912, 3895, 63, 3, 283, 104, 62, 9, 32, 444, 4, 32, 325, 19, 12, 2133, 18, 137, 3, 77, 4, 1551, 81, 690, 3, 6, 427, 3, 21, 930, 1195, 1757, 429, 2, 62, 263, 37, 15, 1, 1520, 2415, 98, 163, 22, 61, 203, 538, 18, 108, 45, 7, 33, 133, 25, 59, 26, 12, 538, 10, 163, 35, 391, 198, 28], [3, 517, 22, 217, 47, 15, 12, 400, 123, 1, 524, 2, 113, 23, 21, 67, 80, 443, 38, 177, 6, 226, 12, 400, 2, 3, 24, 81, 143, 485, 38, 27, 105, 41, 168, 140, 175, 2, 869, 63, 14, 208, 636, 1, 79, 410, 6, 61, 5, 317, 221, 51, 1, 121, 33, 34, 24, 100, 201, 102, 794, 288, 5, 635, 51, 14, 59, 116, 26, 265, 5, 61, 80, 288, 204, 85, 5, 172, 65, 1, 200, 631, 239, 59, 26], [98, 67, 5, 17, 159, 201, 36, 53, 3, 276, 11, 1, 162, 2, 526, 7, 9, 4, 910, 2, 23, 737, 4066, 7, 444, 1, 325, 19, 201, 1086, 44, 20, 120, 55, 704, 344, 17, 28, 9, 341, 44, 20, 120, 4, 618, 1582, 17, 28, 444, 7, 1, 1081, 9, 39, 34, 53, 16, 29, 23, 170, 57, 323, 20, 59, 85, 128, 279, 1101, 47, 4, 4644, 843, 9, 4, 4644, 843, 18, 43, 11, 43, 426, 596]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample sequences:\", sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences shape: (500234, 100)\n",
      "Categorical labels shape: (500234, 3)\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences to ensure uniform input size\n",
    "max_sequence_length = 100  # Adjust this based on your data\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "print(\"Padded sequences shape:\", padded_sequences.shape)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "num_classes = len(np.unique(labels))  # Ensure num_classes is set correctly\n",
    "categorical_labels = to_categorical(labels, num_classes=num_classes)\n",
    "print(\"Categorical labels shape:\", categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences,\n",
    "    categorical_labels,\n",
    "    test_size=0.2,  # 20% of data for validation\n",
    "    random_state=42  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atrav\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m12506/12506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1285s\u001b[0m 102ms/step - accuracy: 0.8445 - loss: 0.4104 - val_accuracy: 0.8815 - val_loss: 0.3090\n",
      "Epoch 2/5\n",
      "\u001b[1m 3080/12506\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:27\u001b[0m 79ms/step - accuracy: 0.8869 - loss: 0.2900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
